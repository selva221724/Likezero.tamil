{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tamilselvan/Documents/Tamil/Likezero.tamil/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "fake_data = pd.read_csv('data/Fake.csv')[:100]\n",
    "true_data = pd.read_csv('data/True.csv')[:100]\n",
    "\n",
    "# Preprocess the data\n",
    "fake_data['label'] = 1  # Fake news label\n",
    "true_data['label'] = 0  # Real news label\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_data = pd.concat([fake_data, true_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>House panel chair introduces $81 billion disas...</td>\n",
       "      <td>WASHINGTON (Reuters) - The chairman of the U.S...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Trump nominates Liberty University professor t...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Trump on Twitter (Dec 18) - Congressional Race...</td>\n",
       "      <td>The following statements were posted to the ve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 18, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Trump Cabinet officials to visit Puerto Rico t...</td>\n",
       "      <td>WASHINGTON (Reuters) - Two members of Presiden...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>'Dreamer' immigration bill not on U.S. Senate ...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Senate will no...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 18, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0    Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1    Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2    Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3    Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4    Pope Francis Just Called Out Donald Trump Dur...   \n",
       "..                                                ...   \n",
       "95  House panel chair introduces $81 billion disas...   \n",
       "96  Trump nominates Liberty University professor t...   \n",
       "97  Trump on Twitter (Dec 18) - Congressional Race...   \n",
       "98  Trump Cabinet officials to visit Puerto Rico t...   \n",
       "99  'Dreamer' immigration bill not on U.S. Senate ...   \n",
       "\n",
       "                                                 text       subject  \\\n",
       "0   Donald Trump just couldn t wish all Americans ...          News   \n",
       "1   House Intelligence Committee Chairman Devin Nu...          News   \n",
       "2   On Friday, it was revealed that former Milwauk...          News   \n",
       "3   On Christmas day, Donald Trump announced that ...          News   \n",
       "4   Pope Francis used his annual Christmas Day mes...          News   \n",
       "..                                                ...           ...   \n",
       "95  WASHINGTON (Reuters) - The chairman of the U.S...  politicsNews   \n",
       "96  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "97  The following statements were posted to the ve...  politicsNews   \n",
       "98  WASHINGTON (Reuters) - Two members of Presiden...  politicsNews   \n",
       "99  WASHINGTON (Reuters) - The U.S. Senate will no...  politicsNews   \n",
       "\n",
       "                  date  label  \n",
       "0    December 31, 2017      1  \n",
       "1    December 31, 2017      1  \n",
       "2    December 30, 2017      1  \n",
       "3    December 29, 2017      1  \n",
       "4    December 25, 2017      1  \n",
       "..                 ...    ...  \n",
       "95  December 19, 2017       0  \n",
       "96  December 19, 2017       0  \n",
       "97  December 18, 2017       0  \n",
       "98  December 19, 2017       0  \n",
       "99  December 18, 2017       0  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    combined_data['text'], combined_data['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Provide the output directory path here\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Change save strategy to match evaluation strategy\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create instances of the datasets\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length=512)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, tokenizer, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tamilselvan/Documents/Tamil/Likezero.tamil/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/240 [00:02<09:52,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6975, 'grad_norm': 2.36472487449646, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|███▎      | 80/240 [00:36<01:05,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.456595242023468, 'eval_runtime': 1.8945, 'eval_samples_per_second': 21.114, 'eval_steps_per_second': 10.557, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 100/240 [00:47<00:58,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5969, 'grad_norm': 12.46007251739502, 'learning_rate': 1e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 160/240 [01:13<00:32,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009769042953848839, 'eval_runtime': 1.9118, 'eval_samples_per_second': 20.923, 'eval_steps_per_second': 10.462, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 200/240 [01:32<00:15,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0626, 'grad_norm': 0.10666202753782272, 'learning_rate': 2e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 240/240 [01:50<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0022792653180658817, 'eval_runtime': 1.9158, 'eval_samples_per_second': 20.879, 'eval_steps_per_second': 10.44, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:53<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 113.7806, 'train_samples_per_second': 4.219, 'train_steps_per_second': 2.109, 'train_loss': 0.27584396830449504, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.27584396830449504, metrics={'train_runtime': 113.7806, 'train_samples_per_second': 4.219, 'train_steps_per_second': 2.109, 'train_loss': 0.27584396830449504, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "Precision: 0.9545454545454546\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9767441860465116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = test_labels.to_numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "fake_data = pd.read_csv('data/Fake.csv')\n",
    "true_data = pd.read_csv('data/True.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "fake_data['label'] = 1  # Fake news label\n",
    "true_data['label'] = 0  # Real news label\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_data = pd.concat([fake_data, true_data])\n",
    "\n",
    "combined_data\n",
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    combined_data['text'], combined_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Provide the output directory path here\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Change save strategy to match evaluation strategy\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Create instances of the datasets\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length=512)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, tokenizer, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = test_labels.to_numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the saved model for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Define source and destination paths\n",
    "source_dir = \"/content/fine_tuned_model\"  # Replace with the path to your saved model directory in Colab\n",
    "destination_dir = \"/content/drive/MyDrive/fakenews/\"  # Replace with the destination directory in your Google Drive\n",
    "\n",
    "# Copy the entire directory to Google Drive\n",
    "shutil.copytree(source_dir, destination_dir + \"fine_tuned_model\")\n",
    "source_dir = \"/content/results\"\n",
    "shutil.copytree(source_dir, destination_dir + \"results\")\n",
    "source_dir = \"/content/logs\"\n",
    "shutil.copytree(source_dir, destination_dir + \"logs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
